{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "from typing import Union\n",
    "\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "import PIL\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    InterpolationMode,\n",
    "    ToImage,\n",
    "    ToDtype,\n",
    "    Normalize,\n",
    ")\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "try:\n",
    "    if is_flash_attn_2_available():\n",
    "        from flash_attn.modules.mha import FlashSelfAttention\n",
    "    else:\n",
    "        FlashSelfAttention = None\n",
    "except ImportError:\n",
    "    FlashSelfAttention = None\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads=16, use_flash_attn=False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        if use_flash_attn and FlashSelfAttention is not None:\n",
    "            self.flash_attn = FlashSelfAttention()\n",
    "        else:\n",
    "            self.flash_attn = None\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(\n",
    "            self.qkv.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
    "        )\n",
    "        torch.nn.init.kaiming_normal_(\n",
    "            self.proj.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.flash_attn is not None:\n",
    "            qkv = self.qkv(x)\n",
    "            qkv = rearrange(\n",
    "                qkv, \"... (three h d) -> ... three h d\", three=3, h=self.num_heads\n",
    "            )\n",
    "            attn_output = self.flash_attn(qkv)\n",
    "            output = rearrange(attn_output, \"... h d -> ... (h d)\")\n",
    "            output = self.proj(output)\n",
    "            return output\n",
    "        else:\n",
    "            B, N, C = x.shape\n",
    "            qkv = (\n",
    "                self.qkv(x)\n",
    "                .reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "                .permute(2, 0, 3, 1, 4)\n",
    "            )\n",
    "            q, k, v = qkv.unbind(0)\n",
    "\n",
    "            x = F.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "            x = x.transpose(1, 2).reshape(B, N, C)\n",
    "            x = self.proj(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "class VitBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, use_flash_attn=False):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(embed_dim, use_flash_attn=use_flash_attn)\n",
    "        self.mlp = MLP(embed_dim, 4304)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, use_flash_attn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        embed_len = 729\n",
    "        embed_dim = 1152\n",
    "\n",
    "        self.patch_embed = LinearPatchEmbedding()\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * 0.02)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[VitBlock(embed_dim, use_flash_attn=use_flash_attn) for _ in range(27)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class EncoderWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, use_flash_attn=False):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict({\"visual\": VisionTransformer(use_flash_attn)})\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model[\"visual\"](x)\n",
    "\n",
    "\n",
    "class LinearPatchEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(588, 1152)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, hp1, wp2 = x.shape\n",
    "        p1, p2 = 14, 14\n",
    "        h, w = hp1 // p1, wp2 // p2\n",
    "        x = x.reshape(b, c, h, p1, w, p2)\n",
    "        x = x.permute(0, 2, 4, 1, 3, 5)\n",
    "        x = x.reshape(b, h * w, c * p1 * p2)\n",
    "\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: int = None,\n",
    "        out_features: int = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU(approximate=\"tanh\")\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(\n",
    "            self.fc1.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
    "        )\n",
    "        torch.nn.init.kaiming_normal_(\n",
    "            self.fc2.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionProjection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        image_embedding_dim = 1152\n",
    "        model_dim = 2048\n",
    "        hidden_dim = model_dim * 4\n",
    "\n",
    "        self.mlp = MLP(image_embedding_dim * 2, hidden_dim, model_dim)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.mlp.fc1.weight.device\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "def create_patches(image, patch_size=(378, 378)):\n",
    "    assert image.dim() == 3, \"Image must be in CHW format\"\n",
    "\n",
    "    _, height, width = image.shape  # Channels, Height, Width\n",
    "    patch_height, patch_width = patch_size\n",
    "\n",
    "    if height == patch_height and width == patch_width:\n",
    "        return []\n",
    "\n",
    "    # Iterate over the image and create patches\n",
    "    patches = []\n",
    "    for i in range(0, height, patch_height):\n",
    "        row_patches = []\n",
    "        for j in range(0, width, patch_width):\n",
    "            patch = image[:, i : i + patch_height, j : j + patch_width]\n",
    "            row_patches.append(patch)\n",
    "        patches.append(torch.stack(row_patches))\n",
    "    return patches\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, use_flash_attn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = EncoderWrapper(use_flash_attn)\n",
    "        self.projection = VisionProjection()\n",
    "        self.supported_sizes = [(378, 378), (378, 756), (756, 378), (756, 756)]\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.projection.mlp.fc1.weight.device\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.projection.mlp.fc1.weight.dtype\n",
    "\n",
    "    def preprocess(self, image: PIL.Image.Image):\n",
    "        width, height = image.size\n",
    "        max_dim = max(width, height)\n",
    "        if max_dim < 512:\n",
    "            im_size = (378, 378)\n",
    "        else:\n",
    "            aspect_ratio = width / height\n",
    "            im_size = min(\n",
    "                self.supported_sizes,\n",
    "                key=lambda size: (\n",
    "                    abs((size[1] / size[0]) - aspect_ratio),\n",
    "                    abs(size[0] - width) + abs(size[1] - height),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        return Compose(\n",
    "            [\n",
    "                Resize(size=im_size, interpolation=InterpolationMode.BICUBIC),\n",
    "                ToImage(),\n",
    "                ToDtype(torch.float32, scale=True),\n",
    "                Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ]\n",
    "        )(image)\n",
    "\n",
    "    def forward(\n",
    "        self, images: Union[PIL.Image.Image, list[PIL.Image.Image], torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        im_list = None\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            # Input must have dimensions (B, C, H, W)\n",
    "            assert (\n",
    "                len(images.shape) == 4\n",
    "            ), \"Tensor input must have dimensions (B, C, H, W)\"\n",
    "            im_list = list(images)\n",
    "        elif isinstance(images, PIL.Image.Image):\n",
    "            im_list = [images]\n",
    "        elif isinstance(images, list):\n",
    "            im_list = images\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Input must be a PIL image, list of PIL images, or a tensor\"\n",
    "            )\n",
    "\n",
    "        # Preprocess unless the images are already tensors (indicating that\n",
    "        # they have already been preprocessed)\n",
    "        if not isinstance(im_list[0], torch.Tensor):\n",
    "            im_list = [self.preprocess(im.convert(\"RGB\")) for im in im_list]\n",
    "\n",
    "        patches = [create_patches(im) for im in im_list]\n",
    "        flat_patches = [patch for image_patches in patches for patch in image_patches]\n",
    "\n",
    "        # Images may be variable size, and need to be resized to a common size after\n",
    "        # creating patches.\n",
    "        resized_images = [\n",
    "            F.interpolate(im.unsqueeze(0), size=(378, 378), mode=\"bilinear\")\n",
    "            for im in im_list\n",
    "        ]\n",
    "\n",
    "        combined_images = torch.cat([*resized_images, *flat_patches], dim=0)\n",
    "        combined_images = combined_images.to(self.device, dtype=self.dtype)\n",
    "\n",
    "        combined_features = self.encoder(combined_images)\n",
    "\n",
    "        full_img_features = combined_features[: len(im_list)]\n",
    "        patch_features = (\n",
    "            combined_features[len(im_list) :].transpose(1, 2).view(-1, 1152, 27, 27)\n",
    "        )\n",
    "\n",
    "        # Reshape patch features back to their original structure\n",
    "        reshaped_patch_features = []\n",
    "        patch_idx = 0\n",
    "        for i, patch_set in enumerate(patches):\n",
    "            if len(patch_set) == 0:\n",
    "                reshaped_patch_features.append(\n",
    "                    full_img_features[i].transpose(0, 1).view(1152, 27, 27)\n",
    "                )\n",
    "            else:\n",
    "                sample_features = []\n",
    "                for row_patches in patch_set:\n",
    "                    row_len = len(row_patches)\n",
    "                    row_features = patch_features[\n",
    "                        patch_idx : patch_idx + row_len\n",
    "                    ]  # row_len, T, C\n",
    "                    row_features = torch.cat(\n",
    "                        list(row_features), dim=2\n",
    "                    )  # T, C * row_len\n",
    "                    patch_idx += row_len\n",
    "                    sample_features.append(row_features)\n",
    "                sample_features = torch.cat(sample_features, dim=1)\n",
    "                sample_features = F.interpolate(\n",
    "                    sample_features.unsqueeze(0), size=(27, 27), mode=\"bilinear\"\n",
    "                ).squeeze(0)\n",
    "                reshaped_patch_features.append(sample_features)\n",
    "        reshaped_patch_features = (\n",
    "            torch.stack(reshaped_patch_features).view(-1, 1152, 729).transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        final_features = torch.cat([full_img_features, reshaped_patch_features], dim=2)\n",
    "\n",
    "        return self.projection(final_features)\n",
    "\n",
    "\n",
    "image = Image.open('download.jpeg')\n",
    "\n",
    "def wow2():\n",
    "  x = VisionEncoder(image)\n",
    "  model_id = \"vikhyatk/moondream2\"\n",
    "  revision = \"2024-08-26\"\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_id, trust_remote_code=True, revision=revision\n",
    "  )\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n",
    "\n",
    "  vision_encoder_2 = model.vision_encoder\n",
    "  original_state_dict = vision_encoder_2.state_dict()\n",
    "\n",
    "  x.load_state_dict(original_state_dict)\n",
    "\n",
    "  torch.save(x.state_dict(), \"model_weights.pth\")\n",
    "\n",
    "  y = x.forward(image)\n",
    "\n",
    "  del x\n",
    "  del model\n",
    "  del tokenizer\n",
    "  del vision_encoder_2\n",
    "  del original_state_dict\n",
    "\n",
    "  return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wow2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7169,  4.5565,  6.6753,  ..., -4.9811, -2.5168, -0.3101],\n",
      "         [ 2.1336,  0.9388,  2.5263,  ..., -0.7272,  1.1775,  1.6335],\n",
      "         [ 1.7289,  0.7734,  1.8790,  ..., -0.5956,  1.2257,  1.4375],\n",
      "         ...,\n",
      "         [ 0.6993,  3.1835,  4.0270,  ..., -2.0063, -0.1059, -0.6612],\n",
      "         [-3.9188,  1.8164,  3.7882,  ...,  6.6367,  2.2692,  1.7847],\n",
      "         [ 1.1831,  2.3517,  3.6561,  ..., -3.2870, -2.4500,  1.9350]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 729, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
