{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xP6RzX8xRCex"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-1M')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "prompt = \"Once upon a time there was\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCA2UP2CRKXV",
        "outputId": "66e0e020-f700-42e4-b8de-dec40b766c60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPTNeoForCausalLM(\n",
            "  (transformer): GPTNeoModel(\n",
            "    (wte): Embedding(50257, 64)\n",
            "    (wpe): Embedding(2048, 64)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=64, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mfKzvRtRV4s",
        "outputId": "af9fbfc8-e593-4a04-e3b6-210d8e627912"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 208])\n",
            "Once upon a time there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, shiny rock in the sky. She wanted to touch it, but it was too high.\n",
            "\n",
            "Lily's mommy told her that it was important to be careful and not touch things. Lily didn't want to touch it, so she asked her mommy if she could touch it. Her mommy said yes and they went to the park.\n",
            "\n",
            "When they got there, Lily saw a big, scary dog. The dog was barking and barked loudly. Lily was scared and didn't know what to do. She tried to run away, but she was too fast. She tried to run away, but it was too late. The dog was too fast and it was too fast.\n",
            "\n",
            "Lily was sad and cried. She wished she had listened to her mommy. She wished she had listened to her mommy and never let her to get back.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate completion\n",
        "output = model.generate(input_ids, max_length = 1000, num_beams=1)\n",
        "print(output.shape)\n",
        "# Decode the completion\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBc4AaZSgK-7"
      },
      "source": [
        "**Model Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QzHdexABhTEz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define model parameters\n",
        "d_model = 64\n",
        "nhead = 8\n",
        "d_hid = 256\n",
        "dropout = 0\n",
        "nlayers = 1\n",
        "sequence_length = 10\n",
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpQ2391Ti0HK"
      },
      "source": [
        "**Transformer Block Pytorch API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmSNP2DTRkHV",
        "outputId": "d924b4af-f69a-4323-be00-58d71c47712b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TransformerEncoder(\n",
            "  (layers): ModuleList(\n",
            "    (0): TransformerEncoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "      (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0, inplace=False)\n",
            "      (dropout2): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Original Transformer Encoder using PyTorch API\n",
        "encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "transformer_encoder = nn.TransformerEncoder(encoder_layer, nlayers)\n",
        "print(transformer_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceFDL6d1hGcU"
      },
      "source": [
        "**Custom Transformer Block**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4IfjK_P3gT87"
      },
      "outputs": [],
      "source": [
        "# Custom Transformer block implementation\n",
        "class CustomTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_hid, dropout=0):\n",
        "        super(CustomTransformerBlock, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)\n",
        "\n",
        "        # Feedforward network\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hid, d_model),\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = x + self.dropout1(attn_output)  # Residual connection\n",
        "        x = self.norm1(x)  # Layer normalization\n",
        "\n",
        "        # Feedforward network with residual connection\n",
        "        ff_output = self.feedforward(x)\n",
        "        x = x + self.dropout2(ff_output)  # Residual connection\n",
        "        x = self.norm2(x)  # Layer normalization\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqmueALDiRKL"
      },
      "source": [
        "**Copying the weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SI0e9T76hKuW"
      },
      "outputs": [],
      "source": [
        "# Instantiate the custom transformer block\n",
        "custom_transformer_block = CustomTransformerBlock(d_model, nhead, d_hid, dropout)\n",
        "\n",
        "# Copy weights from the original transformer layer to the custom transformer block\n",
        "custom_transformer_block.attention.in_proj_weight = encoder_layer.self_attn.in_proj_weight\n",
        "custom_transformer_block.attention.in_proj_bias = encoder_layer.self_attn.in_proj_bias\n",
        "custom_transformer_block.attention.out_proj.weight = encoder_layer.self_attn.out_proj.weight\n",
        "custom_transformer_block.attention.out_proj.bias = encoder_layer.self_attn.out_proj.bias\n",
        "\n",
        "custom_transformer_block.feedforward[0].weight = encoder_layer.linear1.weight\n",
        "custom_transformer_block.feedforward[0].bias = encoder_layer.linear1.bias\n",
        "custom_transformer_block.feedforward[2].weight = encoder_layer.linear2.weight\n",
        "custom_transformer_block.feedforward[2].bias = encoder_layer.linear2.bias\n",
        "\n",
        "custom_transformer_block.norm1.weight = encoder_layer.norm1.weight\n",
        "custom_transformer_block.norm1.bias = encoder_layer.norm1.bias\n",
        "custom_transformer_block.norm2.weight = encoder_layer.norm2.weight\n",
        "custom_transformer_block.norm2.bias = encoder_layer.norm2.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo8--h_Eiai_"
      },
      "source": [
        "**Generating Random Input**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JLNFbFUEiXat"
      },
      "outputs": [],
      "source": [
        "input_tensor = torch.randn(sequence_length, batch_size, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNHB24JsiiqN"
      },
      "source": [
        "**Pass the input through both transformer blocks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FUyFG86hidsZ"
      },
      "outputs": [],
      "source": [
        "original_output = transformer_encoder(input_tensor)\n",
        "custom_output = custom_transformer_block(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKORBBScoZ4s"
      },
      "source": [
        "**Comparing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiUt1Axsim4p",
        "outputId": "8fcd84fb-043d-4132-b3fa-b59e414dfb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Output shape: torch.Size([10, 1, 64])\n",
            "Custom Output shape: torch.Size([10, 1, 64])\n",
            "The outputs are the same!\n"
          ]
        }
      ],
      "source": [
        "print(\"Original Output shape:\", original_output.shape)\n",
        "print(\"Custom Output shape:\", custom_output.shape)\n",
        "\n",
        "# Check if the outputs are close\n",
        "if torch.allclose(original_output, custom_output, atol=1e-6):\n",
        "    print(\"The outputs are the same!\")\n",
        "else:\n",
        "    print(\"The outputs are different.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6W0pDbm4O83"
      },
      "source": [
        "**Original Model Layer Outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpwMm44KoBPZ",
        "outputId": "32cd09b3-a574-4adf-8384-1cf39ec0225d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs: tensor([[7454, 2402,  257,  640,  612,  373,  257]])\n",
            "After Embeddings: torch.Size([1, 7, 64])\n",
            "After Dropout: torch.Size([1, 7, 64])\n",
            "After Block 0: torch.Size([1, 7, 64])\n",
            "After Block 1: torch.Size([1, 7, 64])\n",
            "After Block 2: torch.Size([1, 7, 64])\n",
            "After Block 3: torch.Size([1, 7, 64])\n",
            "After Block 4: torch.Size([1, 7, 64])\n",
            "After Block 5: torch.Size([1, 7, 64])\n",
            "After Block 6: torch.Size([1, 7, 64])\n",
            "After Block 7: torch.Size([1, 7, 64])\n",
            "After Final LayerNorm: torch.Size([1, 7, 64])\n",
            "After LM Head: torch.Size([1, 7, 50257])\n",
            "Generated Text:  upon a time, was a little\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Subclass the original model\n",
        "class GPTNeoForCausalLMWithPrint(GPTNeoForCausalLM):\n",
        "    def forward(self, input_ids, **kwargs):\n",
        "        # Extract the transformer and lm_head from the model\n",
        "        transformer = self.transformer\n",
        "        lm_head = self.lm_head\n",
        "\n",
        "        # Print the input IDs\n",
        "        print(\"Input IDs:\", input_ids)\n",
        "\n",
        "        # Pass input through embeddings\n",
        "        hidden_states = transformer.wte(input_ids) + transformer.wpe(torch.arange(input_ids.shape[-1], device=input_ids.device))\n",
        "        print(\"After Embeddings:\", hidden_states.shape)\n",
        "\n",
        "        # Dropout after embeddings\n",
        "        hidden_states = transformer.drop(hidden_states)\n",
        "        print(\"After Dropout:\", hidden_states.shape)\n",
        "\n",
        "        # Pass through each block (attention + MLP)\n",
        "        for i, block in enumerate(transformer.h):\n",
        "            # Ensure that only the hidden states are returned and passed forward\n",
        "            hidden_states, *_ = block(hidden_states, **kwargs)\n",
        "            print(f\"After Block {i}:\", hidden_states.shape)\n",
        "\n",
        "        # Final layer norm\n",
        "        hidden_states = transformer.ln_f(hidden_states)\n",
        "        print(\"After Final LayerNorm:\", hidden_states.shape)\n",
        "\n",
        "        # Pass through language modeling head\n",
        "        logits = lm_head(hidden_states)\n",
        "        print(\"After LM Head:\", logits.shape)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Load the pretrained model and tokenizer\n",
        "model = GPTNeoForCausalLMWithPrint.from_pretrained('roneneldan/TinyStories-1M')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"Once upon a time there was a\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "output = model(input_ids)\n",
        "\n",
        "# Decode the output to text\n",
        "output_text = tokenizer.decode(output.argmax(dim=-1)[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(\"Generated Text:\", output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tiny Stories Github**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PvByGGVK14-S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the model\n",
            "model loaded...\n",
            "\n",
            "Tom and Mia think of a solution. They say, \"Let's pretend we are wheat. And we can make soup thanked God for us and it bake. We can talk to God, moon.\"\n",
            "\n",
            "They take their shovel and their spades\n",
            "\n",
            "Total generation time: 3.9369 seconds\n",
            "\n",
            "Per-token generation: 0.077193 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "193"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "import time\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "block_size = 64  # what is the maximum context length for predictions?\n",
        "n_embd = 512\n",
        "dropout = 0\n",
        "n_head = 12\n",
        "n_layer = 12\n",
        "# -------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x)  # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(\n",
        "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        # self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(\n",
        "            torch.arange(T, device=device))  # (T,C)\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = enc.n_vocab\n",
        "\n",
        "print(\"Loading the model\")\n",
        "# Load the trained GPT model\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# Load the model with CPU mapping if necessary\n",
        "m.load_state_dict(torch.load(r'./gpt_5_3500.pth', map_location=device))\n",
        "m.eval()\n",
        "print(\"model loaded...\")\n",
        "\n",
        "# Set the initial context for generation\n",
        "text = \"\\n\"  # \"!\"\n",
        "encoded_text = enc.encode(text)\n",
        "context = torch.tensor(encoded_text, dtype=torch.long,\n",
        "                       device=device).unsqueeze(0)\n",
        "# Generate new text\n",
        "start_time = time.time()\n",
        "generated_text = m.generate(context, max_new_tokens=50)[0]\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "per_token_time = total_time / generated_text.size(0)\n",
        "# Decode the generated text from integer indices to characters\n",
        "decoded_text = enc.decode(generated_text.tolist())\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_text)\n",
        "print(f\"\\nTotal generation time: {total_time:.4f} seconds\")\n",
        "print(f\"\\nPer-token generation: {per_token_time:.6f} seconds\")\n",
        "open(r'./generated.txt', 'w').write(decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Giving Initial Tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the model\n",
            "model loaded...\n",
            "Total number of parameters: 89,160,785\n",
            "Initial tokens given to the model: Once upon a time there was a thirsty black crow\n",
            "\n",
            "Generated text\n",
            ":\n",
            "Once upon a time there was a thirsty black crow. Betty wanted to take a drink from the world but it was too deep. She looked around and saw a big green pond with lots of water. She wanted to find the water, but she had no water. \n",
            "\n",
            "Then she saw a little stream in the woods. It was sparkling blue. Betty walked along the stream until she came across the water. She felt so excited that she entered the pond. \n",
            "\n",
            "Betty took the water and they started to play. Amelia found herself swimming around the stream, swimming and laughing. She pretended to swim deep in the water. Then she even jumped under the stream. \n",
            "\n",
            "The pond was so peaceful and quiet. She laughed and laughed all day. \n",
            "\n",
            "Stirp came back whenever Amelia saw it, she was ready to rest. On the way home, Amelia waved goodbye and sang her happy songs. She was so happy that she had finished counting the pond. \n",
            "\n",
            "Just that day she ventured back to discover the pretty, impressive voice again. Again glad she was able to climb the pond.\n",
            "Once there was a dependable giraffe who lived in the forest. He wanted to run as fast as he could. One day, he spotted a giant red button. He ran up to it and picked it up. But, it was stuck! He barked, \"Help me!\"\n",
            "\n",
            "Then he saw a small bird and felt helpless. He wanted to help the bird and said, \"Please help, bird. I am stuck in the tree. We cannot get down.\" The bird said, \"I can help you, and you can reach. Hold on tight!\"\n",
            "\n",
            "The weak bird used his strong wings and flew higher and higher into the tree. He was happy for the big bird. Finally, the strong bird was at the top of the tree. The little bird took the big bird down and thanked the tall tree. From that day on, the small bird stayed strong and did not get tired of helping others.\n",
            "Once upon a time, there was a little girl named Lily. She lived near a big mountain with her family. One day, their favorite toy came to their deep forest. Lily loved her new toy and played with it all day.\n",
            "\n",
            "Soon, it was time for Lily to go home. She didn't want to leave her toy. But her mom and daddy told her not to worry, because sometimes people will be there to help her. Lily was sad\n",
            "\n",
            "Total generation time: 47.1345 seconds\n",
            "\n",
            "Per-token generation: 0.092421 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2126"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "import time\n",
        "\n",
        "# [Your model and class definitions remain the same as above]\n",
        "\n",
        "# Device setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize tokenizer and vocabulary size\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = enc.n_vocab\n",
        "\n",
        "# Load the trained GPT model\n",
        "print(\"Loading the model\")\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "m.load_state_dict(torch.load(r'./gpt_5_3500.pth', map_location=device))\n",
        "m.eval()\n",
        "print(\"model loaded...\")\n",
        "\n",
        "# Calculate and print the total number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")\n",
        "\n",
        "# Set the initial tokens (first two tokens in this case)\n",
        "initial_text = \"Once upon a time there was a thirsty black crow\"  # Replace this with your desired initial text\n",
        "print(\"Initial tokens given to the model:\",initial_text)\n",
        "encoded_initial_text = enc.encode(initial_text)\n",
        "\n",
        "# Convert the encoded text to a tensor and expand dimensions\n",
        "context = torch.tensor(encoded_initial_text, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "# Generate new text starting from the initial context\n",
        "start_time = time.time()\n",
        "generated_text = m.generate(context, max_new_tokens=500)[0]\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate generation time\n",
        "total_time = end_time - start_time\n",
        "per_token_time = total_time / generated_text.size(0)\n",
        "\n",
        "# Decode the generated text from integer indices to characters\n",
        "decoded_text = enc.decode(generated_text.tolist())\n",
        "\n",
        "# Print the generated text\n",
        "print(\"\\nGenerated text\\n:\")\n",
        "print(decoded_text)\n",
        "print(f\"\\nTotal generation time: {total_time:.4f} seconds\")\n",
        "print(f\"\\nPer-token generation: {per_token_time:.6f} seconds\")\n",
        "open(r'./generated.txt', 'w').write(decoded_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tiny Stories With each Layer Output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the model\n",
            "model loaded...\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 5, 512])\n",
            "FeedForward output shape: torch.Size([1, 5, 512])\n",
            "block output: tensor([[[ 0.2531,  2.2977,  0.3734,  ..., -1.8480, -0.2665, -1.4731],\n",
            "         [ 0.7287, -0.6131,  3.7439,  ..., -2.2130,  0.1882,  2.0021],\n",
            "         [-1.1138, -1.2037,  2.3367,  ..., -3.9513,  0.0129,  1.4278],\n",
            "         [ 0.5878, -1.1944,  1.2199,  ..., -1.0795, -1.4784, -1.8418],\n",
            "         [-0.8935, -1.2039,  1.6918,  ...,  0.6740,  1.0122,  0.0944]]],\n",
            "       grad_fn=<AddBackward0>) torch.Size([1, 5, 512])\n",
            "Block output shape: torch.Size([1, 5, 512])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 5, 512])\n",
            "FeedForward output shape: torch.Size([1, 5, 512])\n",
            "block output: tensor([[[ 0.2114,  3.2526,  0.5036,  ..., -1.3900,  1.1721, -2.4607],\n",
            "         [ 0.4264, -0.6603,  4.6477,  ..., -1.1506,  1.9482,  0.6394],\n",
            "         [-1.9144, -2.4571,  3.8895,  ..., -2.7650,  1.4118,  0.6514],\n",
            "         [ 0.7076, -0.8243,  2.4960,  ..., -1.3861, -1.8069, -3.8986],\n",
            "         [-1.9059,  0.0485,  1.4259,  ...,  0.3909,  1.3722, -0.3927]]],\n",
            "       grad_fn=<AddBackward0>) torch.Size([1, 5, 512])\n",
            "Block output shape: torch.Size([1, 5, 512])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 5, 512])\n",
            "FeedForward output shape: torch.Size([1, 5, 512])\n",
            "block output: tensor([[[-0.2764,  3.7850,  2.0733,  ..., -2.7418,  1.1506, -3.9872],\n",
            "         [ 0.5035, -0.7527,  6.2445,  ...,  0.0415,  0.8242, -0.4803],\n",
            "         [-2.6252, -3.0196,  5.5393,  ..., -2.6411,  0.3112, -0.7806],\n",
            "         [ 1.1886, -0.2272,  2.7866,  ..., -2.1562, -2.4092, -5.3747],\n",
            "         [-2.2852,  0.1106, -0.3878,  ...,  0.0394,  1.7513, -0.3460]]],\n",
            "       grad_fn=<AddBackward0>) torch.Size([1, 5, 512])\n",
            "Block output shape: torch.Size([1, 5, 512])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 5, 512])\n",
            "FeedForward output shape: torch.Size([1, 5, 512])\n",
            "block output: tensor([[[-0.1328,  3.8379,  3.3639,  ..., -3.0727,  1.2684, -6.3575],\n",
            "         [-0.3025,  0.9885,  8.1008,  ..., -0.1767, -0.0599, -1.8987],\n",
            "         [-3.2023, -1.9121,  7.6740,  ..., -3.1161, -0.2301, -0.9830],\n",
            "         [ 0.7470, -0.4807,  3.6125,  ..., -3.9435, -2.4733, -6.9498],\n",
            "         [-1.8399,  2.2642, -2.1975,  ..., -1.5769,  2.6149, -1.6877]]],\n",
            "       grad_fn=<AddBackward0>) torch.Size([1, 5, 512])\n",
            "Block output shape: torch.Size([1, 5, 512])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 5, 512])\n",
            "FeedForward output shape: torch.Size([1, 5, 512])\n",
            "block output: tensor([[[-2.7633e+00,  4.5058e+00,  4.4318e+00,  ..., -3.8446e+00,\n",
            "           2.0798e+00, -7.7930e+00],\n",
            "         [-2.4720e+00,  2.8318e+00,  1.0524e+01,  ...,  3.6911e-01,\n",
            "           1.5446e+00, -2.0755e+00],\n",
            "         [-3.9713e+00, -1.0079e+00,  1.0548e+01,  ..., -3.2709e+00,\n",
            "           5.1097e-03, -1.7320e+00],\n",
            "         [ 1.1886e+00, -1.4121e+00,  5.8001e+00,  ..., -4.1592e+00,\n",
            "          -2.9018e+00, -8.7180e+00],\n",
            "         [-2.7410e+00,  3.5092e+00, -2.9339e+00,  ..., -2.9421e+00,\n",
            "           4.0167e+00, -1.3429e+00]]], grad_fn=<AddBackward0>) torch.Size([1, 5, 512])\n",
            "Block output shape: torch.Size([1, 5, 512])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 5, 512])\n",
            "FeedForward output shape: torch.Size([1, 5, 512])\n",
            "block output: tensor([[[ -2.5768,   4.3549,   4.3128,  ...,  -2.5498,   1.9797,  -9.7981],\n",
            "         [ -2.7617,   5.5117,   9.6415,  ...,   2.4265,   1.4958,  -3.8546],\n",
            "         [ -5.0163,   0.8849,  11.5064,  ...,  -2.8627,   0.1959,  -3.7363],\n",
            "         [  0.5796,  -1.1885,   5.7366,  ...,  -3.9872,  -3.0956, -11.6961],\n",
            "         [ -4.3260,   3.2235,  -3.2698,  ...,  -3.4140,   4.3001,  -2.3676]]],\n",
            "       grad_fn=<AddBackward0>) torch.Size([1, 5, 512])\n",
            "Block output shape: torch.Size([1, 5, 512])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 5, 512])\n",
            "FeedForward output shape: torch.Size([1, 5, 512])\n",
            "block output: tensor([[[ -3.2001,   4.5499,   5.2915,  ...,  -1.5811,   3.2254, -11.3823],\n",
            "         [ -4.9924,   6.8042,  10.4080,  ...,   3.6223,   1.5703,  -4.4035],\n",
            "         [ -8.5637,   1.0723,  13.1593,  ...,  -3.4095,   0.6840,  -4.2672],\n",
            "         [ -1.4978,  -1.4245,   7.4331,  ...,  -3.2755,  -2.6623, -14.0049],\n",
            "         [ -5.1475,   2.3070,  -3.7492,  ...,  -2.8635,   4.1747,  -3.5040]]],\n",
            "       grad_fn=<AddBackward0>) torch.Size([1, 5, 512])\n",
            "Block output shape: torch.Size([1, 5, 512])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 5, 512])\n",
            "FeedForward output shape: torch.Size([1, 5, 512])\n",
            "block output: tensor([[[ -3.2831,   6.1675,   6.6215,  ...,  -2.7825,   3.8006, -12.5197],\n",
            "         [ -7.4295,  10.8825,  11.7087,  ...,   3.1829,   2.4061,  -5.2282],\n",
            "         [-10.4972,   3.9545,  16.3143,  ...,  -4.8922,   1.3246,  -5.9609],\n",
            "         [ -1.4655,  -0.9042,   8.9989,  ...,  -4.3896,  -3.2593, -15.5884],\n",
            "         [ -5.2254,   3.0935,  -3.0177,  ...,  -4.5703,   2.6738,  -3.5917]]],\n",
            "       grad_fn=<AddBackward0>) torch.Size([1, 5, 512])\n",
            "Block output shape: torch.Size([1, 5, 512])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 5, 512])\n",
            "FeedForward output shape: torch.Size([1, 5, 512])\n",
            "block output: tensor([[[ -3.5320,   6.6794,   5.4685,  ...,  -1.8939,   5.6992, -15.0053],\n",
            "         [ -7.7112,  12.7515,  11.1272,  ...,   1.6682,   3.2953,  -6.6022],\n",
            "         [-11.2867,   4.8982,  18.4161,  ...,  -5.5165,   1.4809,  -6.7956],\n",
            "         [ -1.3124,  -1.2209,   9.0877,  ...,  -3.8605,  -4.0860, -17.6285],\n",
            "         [ -5.2178,   2.4182,  -2.3954,  ...,  -5.1820,   2.7815,  -3.0093]]],\n",
            "       grad_fn=<AddBackward0>) torch.Size([1, 5, 512])\n",
            "Block output shape: torch.Size([1, 5, 512])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 5, 512])\n",
            "FeedForward output shape: torch.Size([1, 5, 512])\n",
            "block output: tensor([[[ -2.3602,   4.7446,   4.2091,  ...,  -1.2823,   5.1769, -16.5952],\n",
            "         [ -9.1141,  11.8748,  10.7258,  ...,   1.4716,   2.3166,  -6.2503],\n",
            "         [-11.9457,   4.4016,  19.6680,  ...,  -5.8182,   1.4421,  -5.9769],\n",
            "         [ -0.9021,  -1.9901,   8.3421,  ...,  -4.3105,  -3.4872, -20.3137],\n",
            "         [ -6.1277,   1.5061,  -2.2421,  ...,  -6.0336,   2.3432,  -3.9791]]],\n",
            "       grad_fn=<AddBackward0>) torch.Size([1, 5, 512])\n",
            "Block output shape: torch.Size([1, 5, 512])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 5, 512])\n",
            "FeedForward output shape: torch.Size([1, 5, 512])\n",
            "block output: tensor([[[ -2.2391,   4.8992,   2.8078,  ...,  -2.0348,   4.5791, -17.2577],\n",
            "         [ -9.2461,  11.4573,   9.9023,  ...,   0.6644,   0.3171,  -6.8747],\n",
            "         [-11.7683,   5.0704,  21.7951,  ...,  -6.5230,  -1.1412,  -6.2936],\n",
            "         [ -0.5195,  -3.2342,   7.8738,  ...,  -5.9993,  -4.6286, -22.0467],\n",
            "         [ -6.7739,   0.1873,  -1.9666,  ...,  -6.3062,   2.0230,  -5.6012]]],\n",
            "       grad_fn=<AddBackward0>) torch.Size([1, 5, 512])\n",
            "Block output shape: torch.Size([1, 5, 512])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "Head output shape: torch.Size([1, 5, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 5, 512])\n",
            "FeedForward output shape: torch.Size([1, 5, 512])\n",
            "block output: tensor([[[ -2.4380,   4.2319,   2.2260,  ...,  -3.1864,   4.1654, -17.6358],\n",
            "         [ -9.1893,  11.4322,   9.0113,  ...,  -1.7265,   0.1570,  -6.3986],\n",
            "         [-12.6885,   4.6338,  21.8540,  ...,  -8.3654,  -2.1457,  -5.8999],\n",
            "         [ -0.1833,  -3.9365,   7.8537,  ...,  -6.6602,  -5.6203, -22.2243],\n",
            "         [ -6.7595,  -0.4873,  -1.3975,  ...,  -6.3261,   1.3857,  -5.7511]]],\n",
            "       grad_fn=<AddBackward0>) torch.Size([1, 5, 512])\n",
            "Block output shape: torch.Size([1, 5, 512])\n",
            "Logits shape: torch.Size([1, 5, 50257])\n",
            "tensor([[1.0941e-07, 1.2829e-07, 1.1267e-13,  ..., 1.3000e-13, 9.7968e-08,\n",
            "         1.0124e-13]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[373]])\n",
            "Once upon a time there was\n",
            "\n",
            "Total generation time: 0.0723 seconds\n",
            "\n",
            "Per-token generation: 0.012043 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "import time\n",
        "import math\n",
        "\n",
        "# hyperparameters\n",
        "block_size = 64  # what is the maximum context length for predictions?\n",
        "n_embd = 512\n",
        "dropout = 0\n",
        "n_head = 12\n",
        "n_layer = 12\n",
        "# -------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,hs)\n",
        "        q = self.query(x)  # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "\n",
        "        wei = wei.masked_fill(\n",
        "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
        "\n",
        "\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        print(f\"Head output shape: {out.shape}\")\n",
        "        # print(f\"Key weight shape: {self.key.weight.shape}\")\n",
        "        # print(f\"Query weight shape: {self.query.weight.shape}\")\n",
        "        # print(f\"Value weight shape: {self.value.weight.shape}\")\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        print(f\"MultiHeadAttention projected output shape: {out.shape}\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        print(f\"FeedForward output shape: {out.shape}\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        print(\"block output:\", x, x.shape)\n",
        "        print(f\"Block output shape: {x.shape}\")\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        # self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(\n",
        "            torch.arange(T, device=device))  # (T,C)\n",
        "\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        print(f\"Logits shape: {logits.shape}\")\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            print(probs)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            print(idx_next)\n",
        "            # enc = tiktoken.get_encoding(\"gpt2\")\n",
        "            # print(enc.decode([idx_next.item()]))\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = enc.n_vocab\n",
        "\n",
        "print(\"Loading the model\")\n",
        "# Load the trained GPT model\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "\n",
        "# Load the model with CPU mapping if necessary\n",
        "m.load_state_dict(torch.load(r'./gpt_5_3500.pth', map_location=device))\n",
        "m.eval()\n",
        "print(\"model loaded...\")\n",
        "\n",
        "\n",
        "# Set the initial context for generation\n",
        "text = \"Once upon a time there\"  # \"!\"\n",
        "encoded_text = enc.encode(text)\n",
        "context = torch.tensor(encoded_text, dtype=torch.long,\n",
        "                       device=device).unsqueeze(0)\n",
        "\n",
        "# Generate new text\n",
        "start_time = time.time()\n",
        "generated_text = m.generate(context, max_new_tokens=1)[0]\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "per_token_time = total_time / generated_text.size(0)\n",
        "# Decode the generated text from integer indices to characters\n",
        "decoded_text = enc.decode(generated_text.tolist())\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_text)\n",
        "print(f\"\\nTotal generation time: {total_time:.4f} seconds\")\n",
        "print(f\"\\nPer-token generation: {per_token_time:.6f} seconds\")\n",
        "open(r'./generated.txt', 'w').write(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the gamma_beta_dict to a .pth file\n",
        "# torch.save(gamma_beta_dict, 'gamma_beta_values.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
