{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xP6RzX8xRCex"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-1M')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "prompt = \"Once upon a time there was\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCA2UP2CRKXV",
        "outputId": "66e0e020-f700-42e4-b8de-dec40b766c60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPTNeoForCausalLM(\n",
            "  (transformer): GPTNeoModel(\n",
            "    (wte): Embedding(50257, 64)\n",
            "    (wpe): Embedding(2048, 64)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=64, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mfKzvRtRV4s",
        "outputId": "af9fbfc8-e593-4a04-e3b6-210d8e627912"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 208])\n",
            "Once upon a time there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, shiny rock in the sky. She wanted to touch it, but it was too high.\n",
            "\n",
            "Lily's mommy told her that it was important to be careful and not touch things. Lily didn't want to touch it, so she asked her mommy if she could touch it. Her mommy said yes and they went to the park.\n",
            "\n",
            "When they got there, Lily saw a big, scary dog. The dog was barking and barked loudly. Lily was scared and didn't know what to do. She tried to run away, but she was too fast. She tried to run away, but it was too late. The dog was too fast and it was too fast.\n",
            "\n",
            "Lily was sad and cried. She wished she had listened to her mommy. She wished she had listened to her mommy and never let her to get back.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate completion\n",
        "output = model.generate(input_ids, max_length = 1000, num_beams=1)\n",
        "print(output.shape)\n",
        "# Decode the completion\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBc4AaZSgK-7"
      },
      "source": [
        "**Model Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QzHdexABhTEz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define model parameters\n",
        "d_model = 64\n",
        "nhead = 8\n",
        "d_hid = 256\n",
        "dropout = 0\n",
        "nlayers = 1\n",
        "sequence_length = 10\n",
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpQ2391Ti0HK"
      },
      "source": [
        "**Transformer Block Pytorch API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmSNP2DTRkHV",
        "outputId": "d924b4af-f69a-4323-be00-58d71c47712b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TransformerEncoder(\n",
            "  (layers): ModuleList(\n",
            "    (0): TransformerEncoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "      (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0, inplace=False)\n",
            "      (dropout2): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Original Transformer Encoder using PyTorch API\n",
        "encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "transformer_encoder = nn.TransformerEncoder(encoder_layer, nlayers)\n",
        "print(transformer_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceFDL6d1hGcU"
      },
      "source": [
        "**Custom Transformer Block**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4IfjK_P3gT87"
      },
      "outputs": [],
      "source": [
        "# Custom Transformer block implementation\n",
        "class CustomTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_hid, dropout=0):\n",
        "        super(CustomTransformerBlock, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)\n",
        "\n",
        "        # Feedforward network\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hid, d_model),\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = x + self.dropout1(attn_output)  # Residual connection\n",
        "        x = self.norm1(x)  # Layer normalization\n",
        "\n",
        "        # Feedforward network with residual connection\n",
        "        ff_output = self.feedforward(x)\n",
        "        x = x + self.dropout2(ff_output)  # Residual connection\n",
        "        x = self.norm2(x)  # Layer normalization\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqmueALDiRKL"
      },
      "source": [
        "**Copying the weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SI0e9T76hKuW"
      },
      "outputs": [],
      "source": [
        "# Instantiate the custom transformer block\n",
        "custom_transformer_block = CustomTransformerBlock(d_model, nhead, d_hid, dropout)\n",
        "\n",
        "# Copy weights from the original transformer layer to the custom transformer block\n",
        "custom_transformer_block.attention.in_proj_weight = encoder_layer.self_attn.in_proj_weight\n",
        "custom_transformer_block.attention.in_proj_bias = encoder_layer.self_attn.in_proj_bias\n",
        "custom_transformer_block.attention.out_proj.weight = encoder_layer.self_attn.out_proj.weight\n",
        "custom_transformer_block.attention.out_proj.bias = encoder_layer.self_attn.out_proj.bias\n",
        "\n",
        "custom_transformer_block.feedforward[0].weight = encoder_layer.linear1.weight\n",
        "custom_transformer_block.feedforward[0].bias = encoder_layer.linear1.bias\n",
        "custom_transformer_block.feedforward[2].weight = encoder_layer.linear2.weight\n",
        "custom_transformer_block.feedforward[2].bias = encoder_layer.linear2.bias\n",
        "\n",
        "custom_transformer_block.norm1.weight = encoder_layer.norm1.weight\n",
        "custom_transformer_block.norm1.bias = encoder_layer.norm1.bias\n",
        "custom_transformer_block.norm2.weight = encoder_layer.norm2.weight\n",
        "custom_transformer_block.norm2.bias = encoder_layer.norm2.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo8--h_Eiai_"
      },
      "source": [
        "**Generating Random Input**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JLNFbFUEiXat"
      },
      "outputs": [],
      "source": [
        "input_tensor = torch.randn(sequence_length, batch_size, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNHB24JsiiqN"
      },
      "source": [
        "**Pass the input through both transformer blocks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FUyFG86hidsZ"
      },
      "outputs": [],
      "source": [
        "original_output = transformer_encoder(input_tensor)\n",
        "custom_output = custom_transformer_block(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKORBBScoZ4s"
      },
      "source": [
        "**Comparing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiUt1Axsim4p",
        "outputId": "8fcd84fb-043d-4132-b3fa-b59e414dfb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Output shape: torch.Size([10, 1, 64])\n",
            "Custom Output shape: torch.Size([10, 1, 64])\n",
            "The outputs are the same!\n"
          ]
        }
      ],
      "source": [
        "print(\"Original Output shape:\", original_output.shape)\n",
        "print(\"Custom Output shape:\", custom_output.shape)\n",
        "\n",
        "# Check if the outputs are close\n",
        "if torch.allclose(original_output, custom_output, atol=1e-6):\n",
        "    print(\"The outputs are the same!\")\n",
        "else:\n",
        "    print(\"The outputs are different.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6W0pDbm4O83"
      },
      "source": [
        "**Original Model Layer Outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpwMm44KoBPZ",
        "outputId": "32cd09b3-a574-4adf-8384-1cf39ec0225d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs: tensor([[7454, 2402,  257,  640,  612,  373,  257]])\n",
            "After Embeddings: torch.Size([1, 7, 64])\n",
            "After Dropout: torch.Size([1, 7, 64])\n",
            "After Block 0: torch.Size([1, 7, 64])\n",
            "After Block 1: torch.Size([1, 7, 64])\n",
            "After Block 2: torch.Size([1, 7, 64])\n",
            "After Block 3: torch.Size([1, 7, 64])\n",
            "After Block 4: torch.Size([1, 7, 64])\n",
            "After Block 5: torch.Size([1, 7, 64])\n",
            "After Block 6: torch.Size([1, 7, 64])\n",
            "After Block 7: torch.Size([1, 7, 64])\n",
            "After Final LayerNorm: torch.Size([1, 7, 64])\n",
            "After LM Head: torch.Size([1, 7, 50257])\n",
            "Generated Text:  upon a time, was a little\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Subclass the original model\n",
        "class GPTNeoForCausalLMWithPrint(GPTNeoForCausalLM):\n",
        "    def forward(self, input_ids, **kwargs):\n",
        "        # Extract the transformer and lm_head from the model\n",
        "        transformer = self.transformer\n",
        "        lm_head = self.lm_head\n",
        "\n",
        "        # Print the input IDs\n",
        "        print(\"Input IDs:\", input_ids)\n",
        "\n",
        "        # Pass input through embeddings\n",
        "        hidden_states = transformer.wte(input_ids) + transformer.wpe(torch.arange(input_ids.shape[-1], device=input_ids.device))\n",
        "        print(\"After Embeddings:\", hidden_states.shape)\n",
        "\n",
        "        # Dropout after embeddings\n",
        "        hidden_states = transformer.drop(hidden_states)\n",
        "        print(\"After Dropout:\", hidden_states.shape)\n",
        "\n",
        "        # Pass through each block (attention + MLP)\n",
        "        for i, block in enumerate(transformer.h):\n",
        "            # Ensure that only the hidden states are returned and passed forward\n",
        "            hidden_states, *_ = block(hidden_states, **kwargs)\n",
        "            print(f\"After Block {i}:\", hidden_states.shape)\n",
        "\n",
        "        # Final layer norm\n",
        "        hidden_states = transformer.ln_f(hidden_states)\n",
        "        print(\"After Final LayerNorm:\", hidden_states.shape)\n",
        "\n",
        "        # Pass through language modeling head\n",
        "        logits = lm_head(hidden_states)\n",
        "        print(\"After LM Head:\", logits.shape)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Load the pretrained model and tokenizer\n",
        "model = GPTNeoForCausalLMWithPrint.from_pretrained('roneneldan/TinyStories-1M')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"Once upon a time there was a\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "output = model(input_ids)\n",
        "\n",
        "# Decode the output to text\n",
        "output_text = tokenizer.decode(output.argmax(dim=-1)[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(\"Generated Text:\", output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tiny Stories Github**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "PvByGGVK14-S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the model\n",
            "model loaded...\n",
            "\n",
            "Tom and Mia think of a solution. They say, \"Let's pretend we are wheat. And we can make soup thanked God for us and it bake. We can talk to God, moon.\"\n",
            "\n",
            "They take their shovel and their spades\n",
            "\n",
            "Total generation time: 3.7993 seconds\n",
            "\n",
            "Per-token generation: 0.074496 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "193"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "import time\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "block_size = 64  # what is the maximum context length for predictions?\n",
        "n_embd = 512\n",
        "dropout = 0\n",
        "n_head = 12\n",
        "n_layer = 12\n",
        "# -------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x)  # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(\n",
        "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        # self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(\n",
        "            torch.arange(T, device=device))  # (T,C)\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = enc.n_vocab\n",
        "\n",
        "print(\"Loading the model\")\n",
        "# Load the trained GPT model\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# Load the model with CPU mapping if necessary\n",
        "m.load_state_dict(torch.load(r'./gpt_5_3500.pth', map_location=device))\n",
        "m.eval()\n",
        "print(\"model loaded...\")\n",
        "\n",
        "# Set the initial context for generation\n",
        "text = \"\\n\"  # \"!\"\n",
        "encoded_text = enc.encode(text)\n",
        "context = torch.tensor(encoded_text, dtype=torch.long,\n",
        "                       device=device).unsqueeze(0)\n",
        "# Generate new text\n",
        "start_time = time.time()\n",
        "generated_text = m.generate(context, max_new_tokens=50)[0]\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "per_token_time = total_time / generated_text.size(0)\n",
        "# Decode the generated text from integer indices to characters\n",
        "decoded_text = enc.decode(generated_text.tolist())\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_text)\n",
        "print(f\"\\nTotal generation time: {total_time:.4f} seconds\")\n",
        "print(f\"\\nPer-token generation: {per_token_time:.6f} seconds\")\n",
        "open(r'./generated.txt', 'w').write(decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Giving Initial Tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the model\n",
            "model loaded...\n",
            "Total number of parameters: 89,160,785\n",
            "Initial tokens given to the model: A dog named Max\n",
            "\n",
            "Generated text\n",
            ":\n",
            "A dog named Max asked the dog which meant a catch. The dog said, \"Be careful, Timmy!\" Timmy went back to the park to play with his ball. But, he did not see the sign that said \"Danger\".\n",
            "\n",
            "Timmy turned to his mom and said, \"What is that?\" \n",
            "\n",
            "His mom smiled and said, \"It is a sign that means taking care of itself and doing your job well.\" \n",
            "\n",
            "Timmy felt happy and not embarrassed anymore. He knew that praying was important and he promised to be very good at it. From that day on, Timmy knew that Max wasn't alone, and it was important to be kind to his brother.\n",
            "Once upon a time, there was a big whale named Fred. Fred loved to swim in the ocean and play with his friends. One day, Fred met a little girl named Lily.\n",
            "\n",
            "\"Hi, Fred! What are you doing?\" said Lily.\n",
            "\n",
            "\"I am fishing, Lily. I love to be fish on the water!\" said Fred.\n",
            "\n",
            "\"I like fishing too, Fred!\" said Lily. They had so much fun playing catch and catching net. When it was time to go home, they were sad to leave their old friend.\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside with her friends. One day, they went to the park with a picnic and having fun. \n",
            "\n",
            "As they were eating, Lily's mommy told her to stop and let her stop marched on the rope open. Lily got a urge and tried to march away, but her soft moving was stuck. \n",
            "\n",
            "Suddenly, a man got out of the rope. He asked Lily, \"What are you doing in my rope?\" Lily replied, \"I am waving our hands and shaking too!\" The man smiled and said, \"You are crazy!\" And they became good friends. \n",
            "\n",
            "After that, Lily and the man played together and had lots of fun. The man was grateful that they could help someone and before he could handle his body that wasn't only a little bit. And he learned that being kind to people can make them happy.\n",
            "Once upon a time, a little boy named Timmy was playing with his toys in the garden. He had a lot of toys, but his favorite was a While he was playing outside, he saw that his friend Billy came over to play. Timmy\n",
            "\n",
            "Total generation time: 47.1514 seconds\n",
            "\n",
            "Per-token generation: 0.093554 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1987"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "import time\n",
        "\n",
        "# [Your model and class definitions remain the same as above]\n",
        "\n",
        "# Device setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize tokenizer and vocabulary size\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = enc.n_vocab\n",
        "\n",
        "# Load the trained GPT model\n",
        "print(\"Loading the model\")\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "m.load_state_dict(torch.load(r'./gpt_5_3500.pth', map_location=device))\n",
        "m.eval()\n",
        "print(\"model loaded...\")\n",
        "\n",
        "# Calculate and print the total number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")\n",
        "\n",
        "# Set the initial tokens (first two tokens in this case)\n",
        "initial_text = \"A dog named Max\"  # Replace this with your desired initial text\n",
        "print(\"Initial tokens given to the model:\",initial_text)\n",
        "encoded_initial_text = enc.encode(initial_text)\n",
        "\n",
        "# Convert the encoded text to a tensor and expand dimensions\n",
        "context = torch.tensor(encoded_initial_text, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "# Generate new text starting from the initial context\n",
        "start_time = time.time()\n",
        "generated_text = m.generate(context, max_new_tokens=500)[0]\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate generation time\n",
        "total_time = end_time - start_time\n",
        "per_token_time = total_time / generated_text.size(0)\n",
        "\n",
        "# Decode the generated text from integer indices to characters\n",
        "decoded_text = enc.decode(generated_text.tolist())\n",
        "\n",
        "# Print the generated text\n",
        "print(\"\\nGenerated text\\n:\")\n",
        "print(decoded_text)\n",
        "print(f\"\\nTotal generation time: {total_time:.4f} seconds\")\n",
        "print(f\"\\nPer-token generation: {per_token_time:.6f} seconds\")\n",
        "open(r'./generated.txt', 'w').write(decoded_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tiny Stories With each Layer Output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the model\n",
            "model loaded...\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 1, 512])\n",
            "FeedForward output shape: torch.Size([1, 1, 512])\n",
            "Block output shape: torch.Size([1, 1, 512])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 1, 512])\n",
            "FeedForward output shape: torch.Size([1, 1, 512])\n",
            "Block output shape: torch.Size([1, 1, 512])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 1, 512])\n",
            "FeedForward output shape: torch.Size([1, 1, 512])\n",
            "Block output shape: torch.Size([1, 1, 512])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 1, 512])\n",
            "FeedForward output shape: torch.Size([1, 1, 512])\n",
            "Block output shape: torch.Size([1, 1, 512])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 1, 512])\n",
            "FeedForward output shape: torch.Size([1, 1, 512])\n",
            "Block output shape: torch.Size([1, 1, 512])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 1, 512])\n",
            "FeedForward output shape: torch.Size([1, 1, 512])\n",
            "Block output shape: torch.Size([1, 1, 512])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 1, 512])\n",
            "FeedForward output shape: torch.Size([1, 1, 512])\n",
            "Block output shape: torch.Size([1, 1, 512])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 1, 512])\n",
            "FeedForward output shape: torch.Size([1, 1, 512])\n",
            "Block output shape: torch.Size([1, 1, 512])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 1, 512])\n",
            "FeedForward output shape: torch.Size([1, 1, 512])\n",
            "Block output shape: torch.Size([1, 1, 512])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 1, 512])\n",
            "FeedForward output shape: torch.Size([1, 1, 512])\n",
            "Block output shape: torch.Size([1, 1, 512])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 1, 512])\n",
            "FeedForward output shape: torch.Size([1, 1, 512])\n",
            "Block output shape: torch.Size([1, 1, 512])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "Head output shape: torch.Size([1, 1, 42])\n",
            "MultiHeadAttention projected output shape: torch.Size([1, 1, 512])\n",
            "FeedForward output shape: torch.Size([1, 1, 512])\n",
            "Block output shape: torch.Size([1, 1, 512])\n",
            "Logits shape: torch.Size([1, 1, 50257])\n",
            "\n",
            "Tom\n",
            "\n",
            "Total generation time: 0.0447 seconds\n",
            "\n",
            "Per-token generation: 0.022358 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "import time\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "block_size = 64  # what is the maximum context length for predictions?\n",
        "n_embd = 512\n",
        "dropout = 0\n",
        "n_head = 12\n",
        "n_layer = 12\n",
        "# -------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x)  # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(\n",
        "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        print(f\"Head output shape: {out.shape}\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        print(f\"MultiHeadAttention projected output shape: {out.shape}\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        print(f\"FeedForward output shape: {out.shape}\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        print(f\"Block output shape: {x.shape}\")\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        # self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(\n",
        "            torch.arange(T, device=device))  # (T,C)\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        print(f\"Logits shape: {logits.shape}\")\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = enc.n_vocab\n",
        "\n",
        "print(\"Loading the model\")\n",
        "# Load the trained GPT model\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# Load the model with CPU mapping if necessary\n",
        "m.load_state_dict(torch.load(r'./gpt_5_3500.pth', map_location=device))\n",
        "m.eval()\n",
        "print(\"model loaded...\")\n",
        "\n",
        "# Set the initial context for generation\n",
        "text = \"\\n\"  # \"!\"\n",
        "encoded_text = enc.encode(text)\n",
        "context = torch.tensor(encoded_text, dtype=torch.long,\n",
        "                       device=device).unsqueeze(0)\n",
        "# Generate new text\n",
        "start_time = time.time()\n",
        "generated_text = m.generate(context, max_new_tokens=1)[0]\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "per_token_time = total_time / generated_text.size(0)\n",
        "# Decode the generated text from integer indices to characters\n",
        "decoded_text = enc.decode(generated_text.tolist())\n",
        "\n",
        "# Print the generated text\n",
        "print(decoded_text)\n",
        "print(f\"\\nTotal generation time: {total_time:.4f} seconds\")\n",
        "print(f\"\\nPer-token generation: {per_token_time:.6f} seconds\")\n",
        "open(r'./generated.txt', 'w').write(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
