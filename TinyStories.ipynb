{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "xP6RzX8xRCex"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-1M')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "prompt = \"Once upon a time there was\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCA2UP2CRKXV",
        "outputId": "66e0e020-f700-42e4-b8de-dec40b766c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPTNeoForCausalLM(\n",
            "  (transformer): GPTNeoModel(\n",
            "    (wte): Embedding(50257, 64)\n",
            "    (wpe): Embedding(2048, 64)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-7): 8 x GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=64, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mfKzvRtRV4s",
        "outputId": "af9fbfc8-e593-4a04-e3b6-210d8e627912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 208])\n",
            "Once upon a time there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, shiny rock in the sky. She wanted to touch it, but it was too high.\n",
            "\n",
            "Lily's mommy told her that it was important to be careful and not touch things. Lily didn't want to touch it, so she asked her mommy if she could touch it. Her mommy said yes and they went to the park.\n",
            "\n",
            "When they got there, Lily saw a big, scary dog. The dog was barking and barked loudly. Lily was scared and didn't know what to do. She tried to run away, but she was too fast. She tried to run away, but it was too late. The dog was too fast and it was too fast.\n",
            "\n",
            "Lily was sad and cried. She wished she had listened to her mommy. She wished she had listened to her mommy and never let her to get back.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate completion\n",
        "output = model.generate(input_ids, max_length = 1000, num_beams=1)\n",
        "print(output.shape)\n",
        "# Decode the completion\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBc4AaZSgK-7"
      },
      "source": [
        "**Model Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "QzHdexABhTEz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define model parameters\n",
        "d_model = 64\n",
        "nhead = 8\n",
        "d_hid = 256\n",
        "dropout = 0\n",
        "nlayers = 1\n",
        "sequence_length = 10\n",
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpQ2391Ti0HK"
      },
      "source": [
        "**Transformer Block Pytorch API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmSNP2DTRkHV",
        "outputId": "d924b4af-f69a-4323-be00-58d71c47712b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerEncoder(\n",
            "  (layers): ModuleList(\n",
            "    (0): TransformerEncoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "      (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0, inplace=False)\n",
            "      (dropout2): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "# Original Transformer Encoder using PyTorch API\n",
        "encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "transformer_encoder = nn.TransformerEncoder(encoder_layer, nlayers)\n",
        "print(transformer_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceFDL6d1hGcU"
      },
      "source": [
        "**Custom Transformer Block**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "4IfjK_P3gT87"
      },
      "outputs": [],
      "source": [
        "# Custom Transformer block implementation\n",
        "class CustomTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_hid, dropout=0):\n",
        "        super(CustomTransformerBlock, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)\n",
        "\n",
        "        # Feedforward network\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hid, d_model),\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = x + self.dropout1(attn_output)  # Residual connection\n",
        "        x = self.norm1(x)  # Layer normalization\n",
        "\n",
        "        # Feedforward network with residual connection\n",
        "        ff_output = self.feedforward(x)\n",
        "        x = x + self.dropout2(ff_output)  # Residual connection\n",
        "        x = self.norm2(x)  # Layer normalization\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqmueALDiRKL"
      },
      "source": [
        "**Copying the weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "SI0e9T76hKuW"
      },
      "outputs": [],
      "source": [
        "# Instantiate the custom transformer block\n",
        "custom_transformer_block = CustomTransformerBlock(d_model, nhead, d_hid, dropout)\n",
        "\n",
        "# Copy weights from the original transformer layer to the custom transformer block\n",
        "custom_transformer_block.attention.in_proj_weight = encoder_layer.self_attn.in_proj_weight\n",
        "custom_transformer_block.attention.in_proj_bias = encoder_layer.self_attn.in_proj_bias\n",
        "custom_transformer_block.attention.out_proj.weight = encoder_layer.self_attn.out_proj.weight\n",
        "custom_transformer_block.attention.out_proj.bias = encoder_layer.self_attn.out_proj.bias\n",
        "\n",
        "custom_transformer_block.feedforward[0].weight = encoder_layer.linear1.weight\n",
        "custom_transformer_block.feedforward[0].bias = encoder_layer.linear1.bias\n",
        "custom_transformer_block.feedforward[2].weight = encoder_layer.linear2.weight\n",
        "custom_transformer_block.feedforward[2].bias = encoder_layer.linear2.bias\n",
        "\n",
        "custom_transformer_block.norm1.weight = encoder_layer.norm1.weight\n",
        "custom_transformer_block.norm1.bias = encoder_layer.norm1.bias\n",
        "custom_transformer_block.norm2.weight = encoder_layer.norm2.weight\n",
        "custom_transformer_block.norm2.bias = encoder_layer.norm2.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo8--h_Eiai_"
      },
      "source": [
        "**Generating Random Input**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "JLNFbFUEiXat"
      },
      "outputs": [],
      "source": [
        "input_tensor = torch.randn(sequence_length, batch_size, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNHB24JsiiqN"
      },
      "source": [
        "**Pass the input through both transformer blocks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "FUyFG86hidsZ"
      },
      "outputs": [],
      "source": [
        "original_output = transformer_encoder(input_tensor)\n",
        "custom_output = custom_transformer_block(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKORBBScoZ4s"
      },
      "source": [
        "**Comparing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiUt1Axsim4p",
        "outputId": "8fcd84fb-043d-4132-b3fa-b59e414dfb4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Output shape: torch.Size([10, 1, 64])\n",
            "Custom Output shape: torch.Size([10, 1, 64])\n",
            "The outputs are the same!\n"
          ]
        }
      ],
      "source": [
        "print(\"Original Output shape:\", original_output.shape)\n",
        "print(\"Custom Output shape:\", custom_output.shape)\n",
        "\n",
        "# Check if the outputs are close\n",
        "if torch.allclose(original_output, custom_output, atol=1e-6):\n",
        "    print(\"The outputs are the same!\")\n",
        "else:\n",
        "    print(\"The outputs are different.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6W0pDbm4O83"
      },
      "source": [
        "**Original Model Layer Outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpwMm44KoBPZ",
        "outputId": "32cd09b3-a574-4adf-8384-1cf39ec0225d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[7454, 2402,  257,  640,  612,  373,  257]])\n",
            "After Embeddings: torch.Size([1, 7, 64])\n",
            "After Dropout: torch.Size([1, 7, 64])\n",
            "After Block 0: torch.Size([1, 7, 64])\n",
            "After Block 1: torch.Size([1, 7, 64])\n",
            "After Block 2: torch.Size([1, 7, 64])\n",
            "After Block 3: torch.Size([1, 7, 64])\n",
            "After Block 4: torch.Size([1, 7, 64])\n",
            "After Block 5: torch.Size([1, 7, 64])\n",
            "After Block 6: torch.Size([1, 7, 64])\n",
            "After Block 7: torch.Size([1, 7, 64])\n",
            "After Final LayerNorm: torch.Size([1, 7, 64])\n",
            "After LM Head: torch.Size([1, 7, 50257])\n",
            "Generated Text:  upon a time, was a little\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Subclass the original model\n",
        "class GPTNeoForCausalLMWithPrint(GPTNeoForCausalLM):\n",
        "    def forward(self, input_ids, **kwargs):\n",
        "        # Extract the transformer and lm_head from the model\n",
        "        transformer = self.transformer\n",
        "        lm_head = self.lm_head\n",
        "\n",
        "        # Print the input IDs\n",
        "        print(\"Input IDs:\", input_ids)\n",
        "\n",
        "        # Pass input through embeddings\n",
        "        hidden_states = transformer.wte(input_ids) + transformer.wpe(torch.arange(input_ids.shape[-1], device=input_ids.device))\n",
        "        print(\"After Embeddings:\", hidden_states.shape)\n",
        "\n",
        "        # Dropout after embeddings\n",
        "        hidden_states = transformer.drop(hidden_states)\n",
        "        print(\"After Dropout:\", hidden_states.shape)\n",
        "\n",
        "        # Pass through each block (attention + MLP)\n",
        "        for i, block in enumerate(transformer.h):\n",
        "            # Ensure that only the hidden states are returned and passed forward\n",
        "            hidden_states, *_ = block(hidden_states, **kwargs)\n",
        "            print(f\"After Block {i}:\", hidden_states.shape)\n",
        "\n",
        "        # Final layer norm\n",
        "        hidden_states = transformer.ln_f(hidden_states)\n",
        "        print(\"After Final LayerNorm:\", hidden_states.shape)\n",
        "\n",
        "        # Pass through language modeling head\n",
        "        logits = lm_head(hidden_states)\n",
        "        print(\"After LM Head:\", logits.shape)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Load the pretrained model and tokenizer\n",
        "model = GPTNeoForCausalLMWithPrint.from_pretrained('roneneldan/TinyStories-1M')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"Once upon a time there was a\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "output = model(input_ids)\n",
        "\n",
        "# Decode the output to text\n",
        "output_text = tokenizer.decode(output.argmax(dim=-1)[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(\"Generated Text:\", output_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "PvByGGVK14-S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}